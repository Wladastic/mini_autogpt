import json
import os
import traceback
import think.memory as memory
import think.prompt as prompt
import utils.llm as llm
from action.action_decisions import decide
from action.action_execute import take_action
from utils.log import log

DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'

def debug_log(message):
    if DEBUG:
        log(message)

def handle_api_error(response):
    """Handle API error responses"""
    error_msg = f"API returned status code {response.status_code}"
    try:
        error_details = response.json().get('error', response.text)
        error_msg += f": {error_details}"
    except:
        error_msg += f": {response.text}"
    raise Exception(error_msg)

def run_think():
    try:
        thinking = think()
        print("THOUGHTS : " + thinking)
        decision = decide(thinking)
        print("DECISIONS : " + str(decision))
        evaluated_decision = evaluate_decision(thinking, decision)
        print("EVALUATED DECISION : " + str(evaluated_decision))
        take_action(evaluated_decision)
    except Exception as e:
        log(f"Error in thinking process: {e}\n{traceback.format_exc()}")
        raise

def evaluate_decision(thoughts, decision):
    history = llm.build_prompt(prompt.evaluation_prompt)
    history.append({"role": "user", "content": f"Thoughts: {thoughts} \n Decision: {decision}"})
    
    response = llm.llm_request(history)
    if response.status_code != 200:
        handle_api_error(response)
    
    return response.json()["choices"][0]["message"]["content"]

def think():
    """
    Performs the thinking process and returns the thoughts generated by the assistant.
    """
    debug_log("*** I am thinking... ***")
    
    # Build context from history
    history = llm.build_prompt(prompt.thought_prompt)
    thought_history = memory.load_thought_history()
    thought_summaries = [json.loads(item)["summary"] for item in thought_history]

    history = llm.build_context(
        history=history,
        conversation_history=thought_summaries,
        message_history=memory.load_response_history()[-2:],
    )

    history.append({
        "role": "user",
        "content": "Formulate your thoughts and explain them as detailed as you can.",
    })

    # Make request to LLM
    debug_log("Sending request to LLM...")
    response = llm.llm_request(history)
    
    if response.status_code != 200:
        handle_api_error(response)

    thoughts = response.json()["choices"][0]["message"]["content"]
    debug_log("*** Thinking process completed! *** \n")
    memory.save_thought(thoughts, context=history)
    return thoughts
