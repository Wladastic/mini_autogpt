import json
import os
import traceback
import think.memory as memory
import think.prompt as prompt
import utils.llm as llm
from action.action_decisions import decide
from action.action_execute import take_action
from utils.log import log

DEBUG = os.getenv("DEBUG", "false").lower() == "true"


def debug_log(message):
    if DEBUG:
        log(message)


def run_think():
    try:
        thinking = think()
        if thinking is None:
            return
        print("THOUGHTS : " + thinking)
        decision = decide(thinking)
        print("DECISIONS : " + str(decision))
        evaluated_decision = evaluate_decision(thinking, decision)
        print("EVALUATED DECISION : " + str(evaluated_decision))
        take_action(evaluated_decision)
    except Exception as e:
        if DEBUG:
            log(f"Error in thinking process: {e}")
            log(f"Traceback: {traceback.format_exc()}")
        else:
            log(f"Error: {str(e)}")


def evaluate_decision(thoughts, decision):
    history = llm.build_prompt(prompt.evaluation_prompt)
    history.append(
        {"role": "user", "content": f"Thoughts: {thoughts} \n Decision: {decision}"}
    )

    response = llm.llm_request(history)
    if response.status_code != 200:
        if DEBUG:
            debug_log(
                f"Error in evaluate_decision: {response.status_code} - {response.text}"
            )
        return None

    return response.json()["choices"][0]["message"]["content"]


def think():
    """
    Performs the thinking process and returns the thoughts generated by the assistant.
    """
    debug_log("*** I am thinking... ***")

    # Build context from history
    history = llm.build_prompt(prompt.thought_prompt)
    thought_history = memory.load_thought_history()
    thought_summaries = [json.loads(item)["summary"] for item in thought_history]

    history = llm.build_context(
        history=history,
        conversation_history=thought_summaries,
        message_history=memory.load_response_history()[-2:],
    )

    history.append(
        {
            "role": "user",
            "content": "Formulate your thoughts and explain them as detailed as you can.",
        }
    )

    # Make request to LLM
    debug_log("Sending request to LLM...")
    response = llm.llm_request(history)

    if response.status_code != 200:
        if DEBUG:
            debug_log(f"Error in think: {response.status_code} - {response.text}")
        return None

    thoughts = response.json()["choices"][0]["message"]["content"]
    debug_log("*** Thinking process completed! *** \n")
    memory.save_thought(thoughts, context=history)
    return thoughts
